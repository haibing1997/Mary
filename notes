               *************************************************
               **          OpenStack安装笔记                           **
               *************************************************
*****************************************一、基础架构回顾*****************************************************
概念：云操作系统
核心组件：计算（NOVA）、网络（Neutron）、存储(块存储cinder、对象存储swift)
其他（辅助）组件：认证组件（Keystone），镜像组件（Glance）,控制组件（Horizon）
官方文档地址：https://docs.openstack.org/stein/
https://docs.openstack.org/install-guide/
OpenStack 安装节点：
控制节点（Controller）：
计算节点（Compute1）若干个：
存储节点：块存储节点/对象存储节点2
网络节点：
我们做实验就需要两台虚拟机：控制节点、计算节点（存储节点、网络节点）

*****************************************二、硬件的准备**********************************************************
控制节点服务器（虚拟机），配置：4G（最少3G）内存及以上，2块网卡，一个NAT模式，一个仅主机模式，控制节点不需要开启虚拟化功能。
IP（NAT）：192.168.30.11/24
计算节点服务器（虚拟机），配置1G及以上（最好2G以上），2块网卡，一个NAT模式，一个仅主机模式，计算节点必须开启虚拟化功能。
IP（NAT）：192.168.30.31/24
NAT模式子网：192.168.30.0/24
仅主机模式子网：192.168.10.0/24
连接主机时的小技巧：你连接的时候使用仅主机模式的网卡连接，因为暂时用不上仅主机模式的网卡。

===================注意：以下软件环境准备及组件安装每个环节需要做快照，以出现问题时可以进行恢复，快照建议大家在关机状态下做===============
*****************************************三、软件的准备************************************************************
1、准备Openstack的离线tar包
2、迷你云操作系统

*****************************************四、基础环境准备**********************************************************
1、网卡配置(Controller和compute1节点都需要做)：
  Controller：
 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens33
  echo 'IPADDR="192.168.20.11"
  NETMASK="255.255.255.0"
  GATEWAY="192.168.20.2"
  DNS1="192.168.20.2"
  DNS2="114.114.114.114"'>>/etc/sysconfig/network-scripts/ifcfg-ens33 
  systemctl restart network

Compute1：
  sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens33
  echo 'IPADDR="192.168.20.31"
  NETMASK="255.255.255.0"
  GATEWAY="192.168.20.2"
  DNS1="192.168.20.2"
  DNS2="114.114.114.114"'>>/etc/sysconfig/network-scripts/ifcfg-ens33
  systemctl restart network
---------------------------------------------------分割线--------------------------------------------------------------
2、主机名修改：
Controller：
  hostnamectl set-hostname controller
Compute1：
  hostnamectl set-hostname compute1
---------------------------------------------------分割线--------------------------------------------------------------
3、安全设置：
 1）关闭firewalld（Controller和compute1节点都需要执行）：
     systemctl stop firewalld
systemctl disable firewalld
2)Selinux需要关闭及禁用（Controller和compute1节点都需要执行）
    setenforce 0
    sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
---------------------------------------------------分割线--------------------------------------------------------------
4、hosts文件映射：
    echo "192.168.20.11     controller">>/etc/hosts
    echo "192.168.20.31     compute1">>/etc/hosts
---------------------------------------------------分割线--------------------------------------------------------------
5、ntp时间同步（很重要）：
 Controller：
sed -i '4,6d' /etc/chrony.conf
sed -i 's/0.centos.pool.ntp.org/ntp1.aliyun.com/g' /etc/chrony.conf
sed -i 's/#allow 192.168.0.0\/16/allow 192.168.20.0\/24/g' /etc/chrony.conf
Compute1：
sed -i '4,6d' /etc/chrony.conf
sed -i 's/0.centos.pool.ntp.org/controller/g' /etc/chrony.conf
两台服务器都需要重启
systemctl restart chronyd
测试时间是否同步：
chronyc sources
**date命令查看两台虚拟机时间是否同步**
---------------------------------------------------分割线--------------------------------------------------------------
6、tar包上传（两台虚拟机都需要上传Openstack安装包）：
通过xshell文件传输把tar包上传至Centos虚拟机上。
并ll查看是否上传完毕（查看文件大小是否跟源文件一致）。
把上传的文件解压到指定目录（根目录）：
tar -zxvf openstack-pike_aliyun.tar.gz -C /
---------------------------------------------------分割线--------------------------------------------------------------
7、openstack 平台所需yum源配置：
注意：原有的yum源不要动，添加一个local.repo文件
Controller：
echo '[aliyun-os]
name=aliyun-os
baseurl=https://mirrors.aliyun.com/centos/7/os/x86_64/
gpgcheck=0
[openstack]
name=openstack
baseurl=file:///opt/Aliyun-pike/
gpgcheck=0' >/etc/yum.repos.d/local.repo

yum makecache

Compute1：
echo '[aliyun-os]
name=aliyun-os
baseurl=https://mirrors.aliyun.com/centos/7/os/x86_64/
gpgcheck=0
[Virt]
name=CentOS-$releasever - Base
baseurl=http://mirrors.aliyun.com/centos/7/virt/x86_64/kvm-common/
gpgcheck=0
[openstack]
name=openstack
baseurl=file:///opt/Aliyun-pike/
gpgcheck=0' >/etc/yum.repos.d/local.repo


yum makecache
验证以上步骤（注意）
---------------------------------------------------分割线--------------------------------------------------------------

*****************************************五、openstack组件安装 *****************************************
1、基础通用组件安装
 1）基础包安装（*两台虚拟机都需要安装）
   yum install -y python-openstackclient openstack-selinux openstack-utils.noarch 
 
2）数据库（mariadb）（只需要在controller节点上安装）
安装服务 
yum install mariadb mariadb-server python2-PyMySQL -y
设置字符编码及ip地址 
echo '[mysqld]
 bind-address = 192.168.20.11
 default-storage-engine = innodb
 innodb_file_per_table
 max_connections = 4096
 collation-server = utf8_general_ci
 character-set-server = utf8'  >/etc/my.cnf.d/openstack.cnf
启动服务
systemctl start mariadb
systemctl enable mariadb
验证：
systemctl status mariadb
安全设置：
mysql_secure_installation
echo -e "\r\ny\n123456\n123456\ny\ny\ny\ny"| mysql_secure_installation

 3）消息队列（rabbitmq）（只需要在controller节点上安装）
安装
yum install rabbitmq-server -y
启动服务
systemctl enable rabbitmq-server.service
systemctl start rabbitmq-server.service
设置用户及权限
rabbitmqctl add_user openstack RABBIT_PASS
rabbitmqctl set_permissions openstack ".*" ".*" ".*"
启用管理插件
rabbitmq-plugins enable rabbitmq_management
web管理端：
http://192.168.20.11:15672/
账号：guest
密码：guest

 4）缓存（memcached）
安装：
yum install memcached python-memcached -y
修改配置文件：
sed -i 's/-l 127.0.0.1,::1/-l 127.0.0.1,::1,controller/g' /etc/sysconfig/memcached
启动服务：
systemctl enable memcached.service
systemctl start memcached.service
检查是否启动成功：
systemctl status memcached.service
---------------------------------------------------分割线--------------------------------------------------------------

2、KeyStone组件安装
1）创建数据库及授权
mysql -u root -p123456
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
IDENTIFIED BY 'KEYSTONE_DBPASS';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
IDENTIFIED BY 'KEYSTONE_DBPASS';
exit
2）安装软件包：
yum install openstack-keystone httpd mod_wsgi -y
3）修改配置文件：
cp /etc/keystone/keystone.conf{,.bak}
grep -Ev '^$|#' /etc/keystone/keystone.conf.bak >/etc/keystone/keystone.conf
openstack-config --set /etc/keystone/keystone.conf database connection  mysql+pymysql://keystone:KEYSTONE_DBPASS@controller/keystone
openstack-config --set /etc/keystone/keystone.conf token provider  fernet
4）数据同步：
su -s /bin/sh -c "keystone-manage db_sync" keystone
5）安装认证组件：
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
keystone-manage bootstrap --bootstrap-password ADMIN_PASS \
  --bootstrap-admin-url http://controller:35357/v3/ \
  --bootstrap-internal-url http://controller:5000/v3/ \
  --bootstrap-public-url http://controller:5000/v3/ \
  --bootstrap-region-id RegionOne
6）httpd服务中添加服务名及添加连接：
echo "ServerName controller">>/etc/httpd/conf/httpd.conf
ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
7）启动服务
systemctl enable httpd.service
systemctl start httpd.service
8）定义环境变量
echo 'export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=ADMIN_PASS
export OS_AUTH_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2'>admin-openrc
source admin-openrc
9）验证获取token过程
openstack  token issue
10）创建服务
openstack project create --domain default \
  --description "Service Project" service
11）创建角色
openstack role create user
---------------------------------------------------分割线--------------------------------------------------------------

3、Glance组件安装
1）数据库创建及授权
mysql -u root -p123456
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \
  IDENTIFIED BY 'GLANCE_DBPASS';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \
  IDENTIFIED BY 'GLANCE_DBPASS';
exit
**如果时安装完成上一步，进行过重启电脑则要执行下面命令**
source admin-openrcm'y
2）创建用户
openstack user create --domain default --password GLANCE_PASS  glance
3）角色及用户关联
openstack role add --project service --user glance admin
4）创建服务
openstack service create --name glance  --description "OpenStack Image" image
5）创建Endpoint地址
openstack endpoint create --region RegionOne  image public http://controller:9292
openstack endpoint create --region RegionOne  image internal http://controller:9292
openstack endpoint create --region RegionOne  image admin http://controller:9292
6）安装软件包
yum install openstack-glance -y
7）修改配置文件
cp /etc/glance/glance-api.conf{,.bak}
grep '^[a-Z\[]' /etc/glance/glance-api.conf.bak >/etc/glance/glance-api.conf
openstack-config --set /etc/glance/glance-api.conf  database  connection  mysql+pymysql://glance:GLANCE_DBPASS@controller/glance
openstack-config --set /etc/glance/glance-api.conf  glance_store stores  file,http
openstack-config --set /etc/glance/glance-api.conf  glance_store default_store  file
openstack-config --set /etc/glance/glance-api.conf  glance_store filesystem_store_datadir  /var/lib/glance/images/
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken auth_uri  http://controller:5000
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken auth_url  http://controller:35357
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken memcached_servers  controller:11211
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken auth_type  password
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken project_domain_name  default
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken user_domain_name  default
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken project_name  service
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken username  glance
openstack-config --set /etc/glance/glance-api.conf  keystone_authtoken password  GLANCE_PASS
openstack-config --set /etc/glance/glance-api.conf  paste_deploy flavor  keystone

cp /etc/glance/glance-registry.conf{,.bak}
grep '^[a-Z\[]' /etc/glance/glance-registry.conf.bak > /etc/glance/glance-registry.conf
openstack-config --set /etc/glance/glance-registry.conf  database  connection  mysql+pymysql://glance:GLANCE_DBPASS@controller/glance
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken auth_uri  http://controller:5000
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken auth_url  http://controller:35357
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken memcached_servers  controller:11211
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken auth_type  password
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken project_domain_name  default
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken user_domain_name  default
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken project_name  service
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken username  glance
openstack-config --set /etc/glance/glance-registry.conf  keystone_authtoken password  GLANCE_PASS
openstack-config --set /etc/glance/glance-registry.conf  paste_deploy flavor  keystone
8）数据库同步
su -s /bin/sh -c "glance-manage db_sync" glance
9）测试
mysql -u root -p123456 glance -e "show tables;"
10）启动服务
systemctl enable openstack-glance-api.service  openstack-glance-registry.service
systemctl start openstack-glance-api.service  openstack-glance-registry.service

11）上传镜像（提前把镜像文件传到linux虚拟机中）
openstack image create "cirros" \
  --file cirros-0.3.5-x86_64-disk.img \
  --disk-format qcow2 --container-format bare \
  --public
12）查看镜像列表（注意，一定先执行source admin-openrc，特别是系统重启时别忘了执行这个文件）
openstack image list

13）验证
验证安装了哪些组件：
openstack service list
验证endpoint地址
openstack endpoint list
---------------------------------------------------分割线--------------------------------------------------------------
4、NOVA组件安装
#################Controller节点#######################
1）创建数据库及授权
mysql -u root -p123456
CREATE DATABASE nova_api;
CREATE DATABASE nova;
CREATE DATABASE nova_cell0;
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost'  IDENTIFIED BY 'NOVA_DBPASS';
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%'  IDENTIFIED BY 'NOVA_DBPASS';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost'  IDENTIFIED BY 'NOVA_DBPASS';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%'  IDENTIFIED BY 'NOVA_DBPASS';
GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost'  IDENTIFIED BY 'NOVA_DBPASS';
GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' IDENTIFIED BY 'NOVA_DBPASS';
exit

**如果时安装完成上一步，进行过重启电脑则要执行下面命令**
source admin-openrc

2）创建用户及关联角色
openstack user create --domain default --password NOVA_PASS nova
openstack role add --project service --user nova admin
3）创建服务及端点（Ednpoint）
openstack service create --name nova  --description "OpenStack Compute" compute
openstack endpoint create --region RegionOne  compute public http://controller:8774/v2.1
openstack endpoint create --region RegionOne  compute internal http://controller:8774/v2.1
openstack endpoint create --region RegionOne  compute admin http://controller:8774/v2.1
4）Placement服务创建用户及关联角色
openstack user create --domain default --password PLACEMENT_PASS placement
openstack role add --project service --user placement admin
5）创建服务及端点（Endpoint）
openstack service create --name placement --description "Placement API" placement
openstack endpoint create --region RegionOne placement public http://controller:8778
openstack endpoint create --region RegionOne placement internal http://controller:8778
openstack endpoint create --region RegionOne placement admin http://controller:8778
6）安装软件包
yum install -y openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy  openstack-nova-scheduler openstack-nova-placement-api
7）修改配置文件
cp /etc/nova/nova.conf{,.bak}
grep '^[a-Z\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf
openstack-config --set /etc/nova/nova.conf  DEFAULT enabled_apis  osapi_compute,metadata
openstack-config --set /etc/nova/nova.conf  api_database connection  mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api
openstack-config --set /etc/nova/nova.conf  database  connection  mysql+pymysql://nova:NOVA_DBPASS@controller/nova
openstack-config --set /etc/nova/nova.conf  DEFAULT transport_url  rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/nova/nova.conf  api auth_strategy  keystone
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_uri  http://controller:5000
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_url  http://controller:35357
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  memcached_servers  controller:11211
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_type  password
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  user_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_name  service
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  username  nova
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  password  NOVA_PASS
openstack-config --set /etc/nova/nova.conf  DEFAULT my_ip  192.168.20.11
openstack-config --set /etc/nova/nova.conf  DEFAULT use_neutron  True
openstack-config --set /etc/nova/nova.conf  DEFAULT firewall_driver  nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf  vnc enabled  true
openstack-config --set /etc/nova/nova.conf  vnc vncserver_listen  '$my_ip'
openstack-config --set /etc/nova/nova.conf  vnc vncserver_proxyclient_address  '$my_ip'
openstack-config --set /etc/nova/nova.conf  glance api_servers  http://controller:9292
openstack-config --set /etc/nova/nova.conf  oslo_concurrency lock_path  /var/lib/nova/tmp
openstack-config --set /etc/nova/nova.conf  placement os_region_name  RegionOne
openstack-config --set /etc/nova/nova.conf  placement project_domain_name Default
openstack-config --set /etc/nova/nova.conf  placement project_name  service
openstack-config --set /etc/nova/nova.conf  placement auth_type password
openstack-config --set /etc/nova/nova.conf  placement user_domain_name Default
openstack-config --set /etc/nova/nova.conf  placement auth_url  http://controller:35357/v3
openstack-config --set /etc/nova/nova.conf  placement username placement
openstack-config --set /etc/nova/nova.conf  placement password PLACEMENT_PASS

echo '
<Directory /usr/bin>
   <IfVersion >= 2.4>
      Require all granted
   </IfVersion>
   <IfVersion < 2.4>
      Order allow,deny
      Allow from all
   </IfVersion>
</Directory>'  >>/etc/httpd/conf.d/00-nova-placement-api.conf
8）重启HTTPD
systemctl restart httpd
9）数据库同步
su -s /bin/sh -c "nova-manage api_db sync" nova
su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova
su -s /bin/sh -c "nova-manage db sync" nova
nova-manage cell_v2 list_cells
10）启动相关服务
systemctl enable openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service  openstack-nova-conductor.service openstack-nova-novncproxy.service
systemctl start openstack-nova-api.service openstack-nova-consoleauth.service openstack-nova-scheduler.service  openstack-nova-conductor.service openstack-nova-novncproxy.service
11）验证服务
查看安装组件
openstack service list
查看计算节点安装哪些内部服务
openstack compute service list

#################Compute1节点#################
***注意：必须开启虚拟化，基础包先安装***
1）安装软件包
yum install openstack-nova-compute -y
2）修改配置文件
cp /etc/nova/nova.conf{,.bak}
grep '^[a-Z\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf
openstack-config --set /etc/nova/nova.conf  DEFAULT enabled_apis  osapi_compute,metadata
openstack-config --set /etc/nova/nova.conf  DEFAULT transport_url  rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/nova/nova.conf  api auth_strategy  keystone
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_uri  http://controller:5000
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_url  http://controller:35357
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  memcached_servers  controller:11211
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_type  password
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  user_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_name  service
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  username  nova
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  password  NOVA_PASS
openstack-config --set /etc/nova/nova.conf  DEFAULT my_ip  192.168.20.31
openstack-config --set /etc/nova/nova.conf  DEFAULT use_neutron  True
openstack-config --set /etc/nova/nova.conf  DEFAULT firewall_driver  nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf  glance api_servers  http://controller:9292
openstack-config --set /etc/nova/nova.conf  oslo_concurrency lock_path  /var/lib/nova/tmp
openstack-config --set /etc/nova/nova.conf  vnc enabled  True
openstack-config --set /etc/nova/nova.conf  vnc vncserver_listen  0.0.0.0
openstack-config --set /etc/nova/nova.conf  vnc vncserver_proxyclient_address  '$my_ip'
openstack-config --set /etc/nova/nova.conf  vnc novncproxy_base_url  http://controller:6080/vnc_auto.html
openstack-config --set /etc/nova/nova.conf  placement os_region_name  RegionOne
openstack-config --set /etc/nova/nova.conf  placement project_domain_name Default
openstack-config --set /etc/nova/nova.conf  placement project_name  service
openstack-config --set /etc/nova/nova.conf  placement auth_type password
openstack-config --set /etc/nova/nova.conf  placement user_domain_name Default
openstack-config --set /etc/nova/nova.conf  placement auth_url  http://controller:35357/v3
openstack-config --set /etc/nova/nova.conf  placement username placement
openstack-config --set /etc/nova/nova.conf  placement password PLACEMENT_PASS
3）启动相关服务
systemctl enable libvirtd.service openstack-nova-compute.service
systemctl start libvirtd.service openstack-nova-compute.service

***如果出现未能映射主机host错误时执行下面命令（Controller节点）***


su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova
*****做好快照工作*****
---------------------------------------------------分割线--------------------------------------------------------------
5、Neutron组件安装
################Controller节点######################
1）创建数据库及授权
mysql -u root -p123456
CREATE DATABASE neutron;
 GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \
  IDENTIFIED BY 'NEUTRON_DBPASS';
 GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \
  IDENTIFIED BY 'NEUTRON_DBPASS';
exit

**如果时安装完成上一步，进行过重启电脑则要执行下面命令**
source admin-openrc

2）创建用户及角色关联
openstack user create --domain default --password NEUTRON_PASS neutron
openstack role add --project service --user neutron admin
openstack service create --name neutron --description "OpenStack Networking" network
openstack endpoint create --region RegionOne network public http://controller:9696
openstack endpoint create --region RegionOne  network internal http://controller:9696
openstack endpoint create --region RegionOne network admin http://controller:9696
3）安装软件包
yum install openstack-neutron openstack-neutron-ml2  openstack-neutron-linuxbridge ebtables -y
4）修改配置文件
cp /etc/neutron/neutron.conf{,.bak}
grep '^[a-Z\[]' /etc/neutron/neutron.conf.bak >/etc/neutron/neutron.conf
openstack-config --set /etc/neutron/neutron.conf  database connection  mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron
openstack-config --set /etc/neutron/neutron.conf  DEFAULT core_plugin  ml2
openstack-config --set /etc/neutron/neutron.conf  DEFAULT service_plugins
openstack-config --set /etc/neutron/neutron.conf  DEFAULT transport_url  rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/neutron/neutron.conf  DEFAULT auth_strategy  keystone
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_uri  http://controller:5000
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_url  http://controller:35357
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken memcached_servers  controller:11211
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_type  password
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken user_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_name  service
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken username  neutron
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken password  NEUTRON_PASS
openstack-config --set /etc/neutron/neutron.conf  DEFAULT notify_nova_on_port_status_changes  True
openstack-config --set /etc/neutron/neutron.conf  DEFAULT notify_nova_on_port_data_changes  True
openstack-config --set /etc/neutron/neutron.conf  nova auth_url  http://controller:35357
openstack-config --set /etc/neutron/neutron.conf  nova auth_type  password 
openstack-config --set /etc/neutron/neutron.conf  nova project_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  nova user_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  nova region_name  RegionOne
openstack-config --set /etc/neutron/neutron.conf  nova project_name  service
openstack-config --set /etc/neutron/neutron.conf  nova username  nova
openstack-config --set /etc/neutron/neutron.conf  nova password  NOVA_PASS
openstack-config --set /etc/neutron/neutron.conf  oslo_concurrency lock_path  /var/lib/neutron/tmp

cp /etc/neutron/plugins/ml2/ml2_conf.ini{,.bak}
grep '^[a-Z\[]' /etc/neutron/plugins/ml2/ml2_conf.ini.bak >/etc/neutron/plugins/ml2/ml2_conf.ini
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2 type_drivers  flat,vlan
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2 tenant_network_types 
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2 mechanism_drivers  linuxbridge
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2 extension_drivers  port_security
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2_type_flat flat_networks  provider
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  securitygroup enable_ipset  true

cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini{,.bak}
grep '^[a-Z\[]' /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak >/etc/neutron/plugins/ml2/linuxbridge_agent.ini
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  linux_bridge physical_interface_mappings  provider:ens33
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup enable_security_group  True
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup firewall_driver  neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  vxlan enable_vxlan  false

cp /etc/neutron/dhcp_agent.ini{,.bak} 
grep -Ev '^$|#' /etc/neutron/dhcp_agent.ini.bak >/etc/neutron/dhcp_agent.ini
openstack-config --set  /etc/neutron/dhcp_agent.ini  DEFAULT  interface_driver  linuxbridge
openstack-config --set  /etc/neutron/dhcp_agent.ini  DEFAULT dhcp_driver  neutron.agent.linux.dhcp.Dnsmasq
openstack-config --set  /etc/neutron/dhcp_agent.ini  DEFAULT enable_isolated_metadata  true

cp /etc/neutron/metadata_agent.ini{,.bak} 
grep -Ev '^$|#' /etc/neutron/metadata_agent.ini.bak >/etc/neutron/metadata_agent.ini
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT  nova_metadata_ip  controller
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT  metadata_proxy_shared_secret  METADATA_SECRET

openstack-config --set /etc/nova/nova.conf  neutron url  http://controller:9696
openstack-config --set /etc/nova/nova.conf  neutron auth_url  http://controller:35357
openstack-config --set /etc/nova/nova.conf  neutron auth_type  password
openstack-config --set /etc/nova/nova.conf  neutron project_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron user_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron region_name  RegionOne
openstack-config --set /etc/nova/nova.conf  neutron project_name  service
openstack-config --set /etc/nova/nova.conf  neutron username  neutron
openstack-config --set /etc/nova/nova.conf  neutron password  NEUTRON_PASS
openstack-config --set /etc/nova/nova.conf  neutron service_metadata_proxy  true
openstack-config --set /etc/nova/nova.conf  neutron metadata_proxy_shared_secret  METADATA_SECRET

5）创建软连接
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
6）数据库同步
su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf  --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
7）启动相关服务
openstack network agent list
systemctl restart openstack-nova-api.service
systemctl enable neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service  neutron-metadata-agent.service
systemctl start neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
8）验证
数据库是否同步成功
mysql -u root -p123456 neutron -e "show tables;"
查看服务组件是否安装
openstack service list
查看代理是否运行
openstack network agent list

################Compute1节点################
1）安装软件包
yum install openstack-neutron-linuxbridge ebtables ipset -y
2）修改配置文件
cp /etc/neutron/neutron.conf{,.bak}
grep '^[a-Z\[]' /etc/neutron/neutron.conf.bak >/etc/neutron/neutron.conf
openstack-config --set /etc/neutron/neutron.conf  DEFAULT transport_url  rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/neutron/neutron.conf  DEFAULT auth_strategy  keystone
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_uri  http://controller:5000
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_url  http://controller:35357
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken memcached_servers  controller:11211
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_type  password
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken user_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_name  service
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken username  neutron
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken password  NEUTRON_PASS
openstack-config --set /etc/neutron/neutron.conf  oslo_concurrency lock_path  /var/lib/neutron/tmp

cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini{,.bak}
grep '^[a-Z\[]' /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak >/etc/neutron/plugins/ml2/linuxbridge_agent.ini
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  linux_bridge physical_interface_mappings  provider:ens33
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup enable_security_group  true
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup firewall_driver  neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  vxlan enable_vxlan  false

openstack-config --set /etc/neutron/neutron.conf  neutron url  http://controller:9696
openstack-config --set /etc/neutron/neutron.conf  neutron auth_url  http://controller:35357
openstack-config --set /etc/neutron/neutron.conf  neutron auth_type  password
openstack-config --set /etc/neutron/neutron.conf  neutron project_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  neutron user_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  neutron region_name  RegionOne
openstack-config --set /etc/neutron/neutron.conf  neutron project_name  service
openstack-config --set /etc/neutron/neutron.conf  neutron username  neutron
openstack-config --set /etc/neutron/neutron.conf  neutron password  NEUTRON_PASS
3）启动相关服务
systemctl restart openstack-nova-compute.service
systemctl enable neutron-linuxbridge-agent.service
systemctl start neutron-linuxbridge-agent.service

4）验证
查看网络代理（在服务列表中一定要有咱们的compute节点的网桥代理）
 openstack network agent list
---------------------------------------------------分割线--------------------------------------------------------------
6、Horizon组件安装
#######################Controller节点#######################
1）安装软件包
yum install openstack-dashboard -y
2）修改配置文件
cp /etc/openstack-dashboard/local_settings{,.bak}
第38行修改
ALLOWED_HOSTS = ['*']
第183行修改
OPENSTACK_HOST = "controller"
第153行至159行
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
        'LOCATION': 'controller:11211',
    },
}
第186行修改
OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
第75行修改
OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True

第64行至70行修改
OPENSTACK_API_VERSIONS = {
#    "data-processing": 1.1,
     "identity": 3,
     "image": 2,
     "volume": 2,
#    "compute": 2,
}
第90行修改
OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'Default'
第187行修改
OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"
第315行至323行
    'enable_router': False,
    'enable_quotas': False,
    'enable_ipv6': False,
    'enable_distributed_router': False,
    'enable_ha_router': False,
    'enable_lb': False,
    'enable_firewall': False,
    'enable_vpn': False,
    'enable_fip_topology_check': False,
3）启动相关服务
systemctl restart httpd.service memcached.service
4）验证
通过网页访问：http://192.168.30.11/dashboard/
账号：admin
密码：ADMIN_PASS
---------------------------------------------------分割线--------------------------------------------------------------

7、通过Horizon创建网络及创建虚拟机实例
创建虚拟机实例时必备基础
1）计算节点（Nova-compute1）
2）网络必须要有（provider网络/self-service）
3）实例类型（Flaor）
4）虚拟机镜像文件
虚拟机实例创建
1）创建一个provider网络
创建FLAT网络时注意：物理网络名必须跟/etc/neutron/plugins/ml2/ml2_conf.ini文件中flat_networks = provider名称一致
2）创建实例类型
3）上传镜像文件
4）创建虚拟机实例
  *报错问题解决
   （1）虚拟机未映射主机:
        su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova
    （2）虚拟机卡在加载界面：在compute1节点上 /etc/nova/nova.conf文件中 [libvirt] 节点下添加如下代码
      virt_type = qemu
       需要重启nova-compute:systemctl restart  openstack-nova-compute.service
 
  迷你云操作系统登录账号密码：
     账号：cirros  密码：cubswin:)
---------------------------------------------------分割线--------------------------------------------------------------

8、私有网络/租户网络/self-service网络配置
##################Controller节点#####################################
1）准备环境：
对第二块网卡设置静态IP地址
cp /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-ens36
修改ip地址，子网掩码，网关，注意NAME和DEVICE修改为相关的名称ens36，uuid删除或修改一下。
注意：要么重启机器，要么配置一个临时ip地址，不要重启网卡。
ip addr add 192.168.10.11/24 dev ens36

2)修改配置文件
openstack-config --set /etc/neutron/neutron.conf  DEFAULT  service_plugins   router
openstack-config --set /etc/neutron/neutron.conf  DEFAULT allow_overlapping_ips  true
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2   type_drivers  flat,vlan,vxlan
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2   tenant_network_types  vxlan
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2   mechanism_drivers  linuxbridge,l2population
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini  ml2_type_vxlan   vni_ranges  1:1000
openstack-config --set  /etc/neutron/plugins/ml2/linuxbridge_agent.ini   vxlan   enable_vxlan  true
openstack-config --set  /etc/neutron/plugins/ml2/linuxbridge_agent.ini   vxlan   local_ip  192.168.10.11
openstack-config --set  /etc/neutron/plugins/ml2/linuxbridge_agent.ini   vxlan   l2_population   true
cp /etc/neutron/l3_agent.ini{,.bak} 
grep -Ev '^$|#' /etc/neutron/l3_agent.ini.bak >/etc/neutron/l3_agent.ini
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT  interface_driver linuxbridge
3）重启网络服务
systemctl restart neutron-server.service  neutron-linuxbridge-agent.service neutron-dhcp-agent.service   neutron-metadata-agent.service

4）启动三层设备代理
systemctl enable neutron-l3-agent.service
systemctl start neutron-l3-agent.service
5）修改Horizon中配置文件，启用路由功能
vi /etc/openstack-dashboard/local_settings
修改第315行中'enable_router': False,为True
6）重启httpd服务
systemctl restart httpd.service memcached.service


##################Compute1节点#######################
1）对第二块网卡设置静态IP地址
cp /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/sysconfig/network-scripts/ifcfg-ens37
修改ip地址，子网掩码，网关，注意NAME和DEVICE修改为相关的名称ens36，uuid删除或修改一下。
注意：要么重启机器，要么配置一个临时ip地址，不要重启网卡。
ip addr add 192.168.10.31/24 dev ens36
2）修改配置文件
openstack-config --set   /etc/neutron/plugins/ml2/linuxbridge_agent.ini   vxlan   enable_vxlan  true
openstack-config --set  /etc/neutron/plugins/ml2/linuxbridge_agent.ini   vxlan   local_ip  192.168.10.31
openstack-config --set  /etc/neutron/plugins/ml2/linuxbridge_agent.ini   vxlan   l2_population   true
3）重启网桥代理
systemctl restart neutron-linuxbridge-agent.service
4）验证
在Controller节点上执行
openstack network agent list

比原来多了L3-Agent的服务即可
85c3067d-4c98-4a38-9b37-8090fa7344ec | L3 agent           | controller | nova              | :-)   | UP    | neutron-l3-agent   

---------------------------------------------------分割线--------------------------------------------------------------

9、块存储设备Cinder组件安装
##################Controller节点##################
1）数据库创库授权
mysql -u root -p123456
CREATE DATABASE cinder;
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost'  IDENTIFIED BY 'CINDER_DBPASS';
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%'  IDENTIFIED BY 'CINDER_DBPASS';
exit
2）添加用户及关联角色
openstack user create --domain default --password CINDER_PASS cinder
openstack role add --project service --user cinder admin
3）创建服务及注册endpoint地址
openstack service create --name cinderv2  --description "OpenStack Block Storage" volumev2
openstack service create --name cinderv3  --description "OpenStack Block Storage" volumev3
openstack endpoint create --region RegionOne volumev2 public http://controller:8776/v2/%\(project_id\)s
openstack endpoint create --region RegionOne  volumev2 internal http://controller:8776/v2/%\(project_id\)s
openstack endpoint create --region RegionOne volumev2 admin http://controller:8776/v2/%\(project_id\)s
openstack endpoint create --region RegionOne  volumev3 public http://controller:8776/v3/%\(project_id\)s
openstack endpoint create --region RegionOne  volumev3 internal http://controller:8776/v3/%\(project_id\)s
openstack endpoint create --region RegionOne  volumev3 admin http://controller:8776/v3/%\(project_id\)s
4）安装服务相应软件包
yum install -y openstack-cinder
5）修改相应服务的配置文件
cp /etc/cinder/cinder.conf{,.bak}
grep -Ev '^$|#' /etc/cinder/cinder.conf.bak >/etc/cinder/cinder.conf
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  transport_url  rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  auth_strategy  keystone
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  my_ip  192.168.30.11
openstack-config --set /etc/cinder/cinder.conf   database connection mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   auth_uri  http://controller:5000
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   auth_url  http://controller:35357
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   memcached_servers  controller:11211
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   auth_type  password
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   project_domain_name  default
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   user_domain_name  default
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   project_name  service
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   username  cinder
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   password  CINDER_PASS
openstack-config --set /etc/cinder/cinder.conf   oslo_concurrency  lock_path  /var/lib/cinder/tmp
openstack-config --set  /etc/nova/nova.conf   cinder  os_region_name  RegionOne
6）同步数据库
su -s /bin/sh -c "cinder-manage db sync" cinder
7）启动服务
systemctl restart openstack-nova-api.service
systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service
systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service
8）验证
查看服务是否正常
openstack service list
查看卷服务是否正常


##################Compute1节点##################
1）增加一块硬盘
echo '- - -' >/sys/class/scsi_host/host0/scan 
fdisk -l
2）安装服务并启动服务
yum install -y lvm2 device-mapper-persistent-data
systemctl enable lvm2-lvmetad.service
systemctl start lvm2-lvmetad.service
3）创建物理卷
pvcreate /dev/sdb
4）创建卷组
vgcreate lvm-volumes /dev/sdb
5）修改LVM配置
###修改/etc/lvm/lvm.conf
filter = [ "a/sdb/","r/.*/"]
6）安装软件
yum install openstack-cinder targetcli python-keystone -y
7）修改配置文件
cp /etc/cinder/cinder.conf{,.bak}
grep -Ev '^$|#' /etc/cinder/cinder.conf.bak >/etc/cinder/cinder.conf
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  transport_url  rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  auth_strategy  keystone
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  my_ip  192.168.30.31
openstack-config --set /etc/cinder/cinder.conf   DEFAULT  enabled_backends lvm
openstack-config --set /etc/cinder/cinder.conf   database connection mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   auth_uri  http://controller:5000
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   auth_url  http://controller:35357
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   memcached_servers  controller:11211
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   auth_type  password
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   project_domain_name  default
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   user_domain_name  default
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   project_name  service
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   username  cinder
openstack-config --set /etc/cinder/cinder.conf   keystone_authtoken   password  CINDER_PASS
openstack-config --set /etc/cinder/cinder.conf   oslo_concurrency  lock_path  /var/lib/cinder/tmp
echo '[lvm]'>>/etc/cinder/cinder.conf
openstack-config --set /etc/cinder/cinder.conf   lvm  volume_driver  cinder.volume.drivers.lvm.LVMVolumeDriver
openstack-config --set /etc/cinder/cinder.conf   lvm  volume_group lvm-volumes
openstack-config --set /etc/cinder/cinder.conf   lvm  iscsi_protocol   iscsi
openstack-config --set /etc/cinder/cinder.conf   lvm  iscsi_helper  lioadm
8）启动相关服务
systemctl enable openstack-cinder-volume.service target.service
systemctl start openstack-cinder-volume.service target.service
9）验证
查看服务是否启动
systemctl status openstack-cinder-volume.service target.service
10）排查错误
   a)服务状态为down的原因：时间不同步导致，解决方案，把时间同步一下
   b)卷组跟配置文件不一致导致target起不来。解决方案，把名称修改为一致/

11）再添加一个卷组
  a)增加一块硬盘
    echo '- - -' >/sys/class/scsi_host/host0/scan 
    fdisk -l
 b）创建物理卷
     pvcreate /dev/sdd
 c）修改LVM配置
  ###修改/etc/lvm/lvm.conf
  filter = [ "a/sdb/","a/sdd/","r/.*/"]
 c）创建卷组
     vgcreate ssd-volumes /dev/sdd
 d）修改配置文件
    vi /etc/cinder/cinder.conf
     添加一个ssd的节点
      [DEFAULT]
     enabled_backends = lvm,ssc
      [ssc]
     volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
     volume_group = ssc-volumes
     iscsi_protocol = iscsi
     iscsi_helper = lioadm
12）选择指定的卷组
    a)修改配置文件
     vi /etc/cinder/cinder.conf
    [ssd]节点和[lvm]节点下各自添加如下代码
    [lvm] 
     volume_backend_name = lvm
    [ssd]
     volume_backend_name = ssc
     b)重启服务
     systemctl restart openstack-cinder-volume.service target.service
    b)在horizon中卷类型进行设置
 
---------------------------------------------------分割线--------------------------------------------------------------
10、对象储SWIFT组件安装
##################Controller节点##################
1）创建用户及角色关联
openstack user create --domain default --password SWIFT_PASS  swift
openstack role add --project service --user swift admin
2）创建服务及Endpoint
openstack service create --name swift  --description "OpenStack Object Storage" object-store
openstack endpoint create --region RegionOne   object-store public http://controller:8080/v1/AUTH_%\(project_id\)s
openstack endpoint create --region RegionOne  object-store internal http://controller:8080/v1/AUTH_%\(project_id\)s
openstack endpoint create --region RegionOne  object-store admin http://controller:8080/v1
3）安装软件包
yum install openstack-swift-proxy python-swiftclient  python-keystoneclient python-keystonemiddleware   -y
4）下载配置文件模板并存放与指定路径
 安装wget：yum install -y wget
wget https://git.openstack.org/cgit/openstack/swift/plain/etc/proxy-server.conf-sample?h=stable/pike
mv proxy-server.conf-sample\?h\=stable%2Fpike /etc/swift/proxy-server.conf
5）修改配置文件
vi /etc/swift/proxy-server.conf
第6行和第7行把注释去掉
swift_dir = /etc/swift
user = swift
第97行修改为
pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server
第148行修改为
account_autocreate = True
第386行、387和409行注释去掉
operator_roles = admin, user
第361和362行注释去掉
第363行和364行修改为
auth_uri = http://controller:5000
auth_url = http://controller:35357
在364行下面添加如下：
memcached_servers = controller:11211
auth_type = password
project_domain_id = default 
user_domain_id = default 
project_name = service 
username = swift 
password = SWIFT_PASS
delay_auth_decision = True
第475行修改为：
memcache_servers = controller:11211

##################Storage节点##################
1）添加两块硬盘并扫描
echo "- - -" >>/sys/class/scsi_host/host0/scan
2）格式化磁盘
mkfs.xfs /dev/sdd
mkfs.xfs /dev/sde
3）创建挂载点并挂载
mkdir -p /srv/node/sdd
mkdir -p /srv/node/sde
mount /dev/sdd /srv/node/sdd
mount /dev/sde  /srv/node/sde
4)开机自动挂载
echo '/dev/sdd /srv/node/sdb xfs noatime,nodiratime,nobarrier,logbufs=8 0 2
/dev/sde /srv/node/sdc xfs noatime,nodiratime,nobarrier,logbufs=8 0 2'>>/etc/fstab 
5)安装软件包
yum install -y xfsprogs rsync
6)配置同步软件的配置文件
echo '
uid = swift
gid = swift
log file = /var/log/rsyncd.log
pid file = /var/run/rsyncd.pid
address = 192.168.30.31

[account]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/account.lock

[container]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/container.lock

[object]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/object.lock' >>/etc/rsyncd.conf 
7)启动相关服务
systemctl enable rsyncd.service
systemctl start rsyncd.service

8）安装软件包
yum install  -y openstack-swift-account openstack-swift-container  openstack-swift-object
9）修改配置文件
   a）安装wget：yum install -y wget
   b)下载配置文件模板
     wget https://git.openstack.org/cgit/openstack/swift/plain/etc/account-server.conf-sample?h=stable/pike
     wget https://git.openstack.org/cgit/openstack/swift/plain/etc/container-server.conf-sample?h=stable/pike
     wget https://git.openstack.org/cgit/openstack/swift/plain/etc/object-server.conf-sample?h=stable/pike
   c）移动模板文件至指定路径
     mv account-server.conf-sample\?h\=stable%2Fpike /etc/swift/account-server.conf
     mv container-server.conf-sample\?h\=stable%2Fpike /etc/swift/container-server.conf
     mv object-server.conf-sample\?h\=stable%2Fpike /etc/swift/object-server.conf
    d）修改配置文件/etc/swift/account-server.conf
     第2行修改为：
      bind_ip = 192.168.30.31
     第6行至第9行注释去掉
     第109行注释去掉
    e）修改配置文件：/etc/swift/container-server.conf
      第2行修改为：
        bind_ip = 192.168.30.31
      第6行至第9行注释去掉
      第118行注释去掉
   f）修改配置文件：/etc/swift/object-server.conf
        第2行修改为：
        bind_ip = 192.168.30.31
        第6行至第9行注释去掉
        第210行、211行注释去掉
10）修改挂载点目录权限（所属主、所属组）
chown -R swift:swift /srv/node
11）创建缓存目录及设置权限
mkdir -p /var/cache/swift
chown -R root:swift /var/cache/swift
chmod -R 775 /var/cache/swift

##################Controller节点##################
1)创建账户Ring文件，并把所有存储节点添加到Ring中
 cd  /etc/swift
swift-ring-builder account.builder create 10 1 1
swift-ring-builder account.builder  add --region 1 --zone 1 --ip 192.168.30.31 --port 6202  --device sdd --weight 100
swift-ring-builder account.builder  add --region 1 --zone 1 --ip 192.168.30.31 --port 6202  --device sde --weight 100
swift-ring-builder account.builder rebalance
swift-ring-builder account.builder
2）创建容器Ring文件，并把所有存储节点添加到Ring中
swift-ring-builder container.builder create 10 1 1
swift-ring-builder container.builder add --region 1 --zone 1 --ip 192.168.30.31 --port 6201 --device sdd --weight 100
swift-ring-builder container.builder add  --region 1 --zone 1 --ip 192.168.30.31 --port 6201 --device sde --weight 100
swift-ring-builder container.builder rebalance
swift-ring-builder container.builder
3）创建对象Ring文件，并把所有存储节点添加到Ring中
swift-ring-builder object.builder create 10 1 1
swift-ring-builder object.builder add  --region 1 --zone 1 --ip 192.168.30.31 --port 6200 --device sdd --weight 100
swift-ring-builder object.builder add  --region 1 --zone 1 --ip 192.168.30.31 --port 6200 --device sde --weight 100
swift-ring-builder object.builder rebalance
swift-ring-builder object.builder
4）把所有生成的账户文件拷贝到存储节点上
scp *.gz root@compute1:/etc/swift/
5）下载swift.conf配置文件并配置
wget https://git.openstack.org/cgit/openstack/swift/plain/etc/swift.conf-sample?h=stable/pike
mv swift.conf-sample\?h\=stable%2Fpike /etc/swift/swift.conf
vi /etc/swift/swift.conf
[swift-hash]
swift_hash_path_suffix = HASH_PATH_SUFFIX
swift_hash_path_prefix = HASH_PATH_PREFIX
6）把该文件拷贝到所有代理节点及存储节点上
scp /etc/swift/swift.conf root@compute1:/etc/swift/
7）修改配置文件权限
chown -R root:swift /etc/swift
8）启动相关服务
systemctl enable openstack-swift-proxy.service
systemctl restart openstack-swift-proxy.service 

##################Storage节点##################
1）修改配置文件目录权限
chown -R root:swift /etc/swift
2）启动相关服务
systemctl enable openstack-swift-account.service openstack-swift-account-auditor.service  openstack-swift-account-reaper.service openstack-swift-account-replicator.service
systemctl start openstack-swift-account.service openstack-swift-account-auditor.service  openstack-swift-account-reaper.service openstack-swift-account-replicator.service
systemctl enable openstack-swift-container.service  openstack-swift-container-auditor.service openstack-swift-container-replicator.service  openstack-swift-container-updater.service
systemctl start openstack-swift-container.service  openstack-swift-container-auditor.service openstack-swift-container-replicator.service  openstack-swift-container-updater.service
systemctl enable openstack-swift-object.service openstack-swift-object-auditor.service  openstack-swift-object-replicator.service openstack-swift-object-updater.service
systemctl start openstack-swift-object.service openstack-swift-object-auditor.service  openstack-swift-object-replicator.service openstack-swift-object-updater.service

##################Controller节点##################
验证
swift stat
---------------------------------------------------分割线--------------------------------------------------------------

11、添加Compute2节点
##################Compute2节点##################
***注意：必须开启虚拟化，基础包先安装***
1）配置网络
  sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens33
  echo 'IPADDR="192.168.30.41"
  NETMASK="255.255.255.0"
  GATEWAY="192.168.30.2"
  DNS1="192.168.30.2"
  DNS2="114.114.114.114"'>>/etc/sysconfig/network-scripts/ifcfg-ens33
  复制ens33文件为ens36并修改里面配置
  systemctl restart network
2）主机名修改：
  hostnamectl set-hostname compute2
3）安全设置：
 a）关闭firewalld：
     systemctl stop firewalld
     systemctl disable firewalld
  b）Selinux需要关闭及禁用
    setenforce 0
    sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
4）hosts文件映射：
    echo "192.168.30.11     controller">>/etc/hosts
    echo "192.168.30.31     compute1">>/etc/hosts
    echo "192.168.30.41     compute2">>/etc/hosts（全部虚拟机都需要做）
5）ntp时间同步（很重要）：
sed -i '4,6d' /etc/chrony.conf
sed -i 's/0.centos.pool.ntp.org/controller/g' /etc/chrony.conf
systemctl restart chronyd
测试时间是否同步：
chronyc sources
**date命令查看两台虚拟机时间是否同步**
6）tar包上传（两台虚拟机都需要上传Openstack安装包）：
通过xshell文件传输把tar包上传至Centos虚拟机上。
并ll查看是否上传完毕（查看文件大小是否跟源文件一致）。
把上传的文件解压到指定目录（根目录）：
tar -zxvf openstack-pike_aliyun.tar.gz -C /
7）openstack 平台所需yum源配置：
注意：原有的yum源不要动，添加一个local.repo文件
echo '[aliyun-os]
name=aliyun-os
baseurl=https://mirrors.aliyun.com/centos/7/os/x86_64/
gpgcheck=0
[Virt]
name=CentOS-$releasever - Base
baseurl=http://mirrors.aliyun.com/centos/7/virt/x86_64/kvm-common/
gpgcheck=0
[openstack]
name=openstack
baseurl=file:///opt/Aliyun-pike/
gpgcheck=0' >/etc/yum.repos.d/local.repo

yum clean all
yum makecache
8）验证以上步骤（注意）

9）安装软件包
yum install -y python-openstackclient openstack-selinux openstack-utils.noarch  openstack-nova-compute  openstack-neutron-linuxbridge ebtables ipset 
10）修改配置文件
cp /etc/nova/nova.conf{,.bak}
grep '^[a-Z\[]' /etc/nova/nova.conf.bak >/etc/nova/nova.conf
openstack-config --set /etc/nova/nova.conf  DEFAULT enabled_apis  osapi_compute,metadata
openstack-config --set /etc/nova/nova.conf  DEFAULT transport_url  rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/nova/nova.conf  DEFAULT my_ip  192.168.30.41
openstack-config --set /etc/nova/nova.conf  DEFAULT use_neutron  True
openstack-config --set /etc/nova/nova.conf  DEFAULT firewall_driver  nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf  api auth_strategy  keystone
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_uri  http://controller:5000
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_url  http://controller:35357
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  memcached_servers  controller:11211
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  auth_type  password
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  user_domain_name  default
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  project_name  service
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  username  nova
openstack-config --set /etc/nova/nova.conf  keystone_authtoken  password  NOVA_PASS
openstack-config --set /etc/nova/nova.conf  glance api_servers  http://controller:9292
openstack-config --set /etc/nova/nova.conf  oslo_concurrency lock_path  /var/lib/nova/tmp
openstack-config --set /etc/nova/nova.conf  vnc enabled  True
openstack-config --set /etc/nova/nova.conf  vnc vncserver_listen  0.0.0.0
openstack-config --set /etc/nova/nova.conf  vnc vncserver_proxyclient_address  '$my_ip'
openstack-config --set /etc/nova/nova.conf  vnc novncproxy_base_url  http://controller:6080/vnc_auto.html
openstack-config --set /etc/nova/nova.conf  placement os_region_name  RegionOne
openstack-config --set /etc/nova/nova.conf  placement project_domain_name Default
openstack-config --set /etc/nova/nova.conf  placement project_name  service
openstack-config --set /etc/nova/nova.conf  placement auth_type password
openstack-config --set /etc/nova/nova.conf  placement user_domain_name Default
openstack-config --set /etc/nova/nova.conf  placement auth_url  http://controller:35357/v3
openstack-config --set /etc/nova/nova.conf  placement username placement
openstack-config --set /etc/nova/nova.conf  placement password PLACEMENT_PASS
openstack-config --set /etc/nova/nova.conf  libvirt  virt_type  qemu
cp /etc/neutron/neutron.conf{,.bak}
grep '^[a-Z\[]' /etc/neutron/neutron.conf.bak >/etc/neutron/neutron.conf
openstack-config --set /etc/neutron/neutron.conf  DEFAULT transport_url  rabbit://openstack:RABBIT_PASS@controller
openstack-config --set /etc/neutron/neutron.conf  DEFAULT auth_strategy  keystone
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_uri  http://controller:5000
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_url  http://controller:35357
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken memcached_servers  controller:11211
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken auth_type  password
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken user_domain_name  default
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken project_name  service
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken username  neutron
openstack-config --set /etc/neutron/neutron.conf  keystone_authtoken password  NEUTRON_PASS
openstack-config --set /etc/neutron/neutron.conf  oslo_concurrency lock_path  /var/lib/neutron/tmp
openstack-config --set /etc/nova/nova.conf  neutron url  http://controller:9696
openstack-config --set /etc/nova/nova.conf  neutron auth_url  http://controller:35357
openstack-config --set /etc/nova/nova.conf  neutron auth_type  password
openstack-config --set /etc/nova/nova.conf  neutron project_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron user_domain_name  default
openstack-config --set /etc/nova/nova.conf  neutron region_name  RegionOne
openstack-config --set /etc/nova/nova.conf  neutron project_name  service
openstack-config --set /etc/nova/nova.conf  neutron username  neutron
openstack-config --set /etc/nova/nova.conf  neutron password  NEUTRON_PASS
cp /etc/neutron/plugins/ml2/linuxbridge_agent.ini{,.bak}
grep '^[a-Z\[]' /etc/neutron/plugins/ml2/linuxbridge_agent.ini.bak >/etc/neutron/plugins/ml2/linuxbridge_agent.ini
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  linux_bridge physical_interface_mappings  provider:ens33
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup enable_security_group  true
openstack-config --set /etc/neutron/plugins/ml2/linuxbridge_agent.ini  securitygroup firewall_driver  neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
openstack-config --set   /etc/neutron/plugins/ml2/linuxbridge_agent.ini   vxlan   enable_vxlan  true
openstack-config --set  /etc/neutron/plugins/ml2/linuxbridge_agent.ini   vxlan   local_ip  192.168.10.41
openstack-config --set  /etc/neutron/plugins/ml2/linuxbridge_agent.ini   vxlan   l2_population   true 
11）启动相关服务
systemctl enable libvirtd.service openstack-nova-compute.service neutron-linuxbridge-agent.service
systemctl start libvirtd.service openstack-nova-compute.service neutron-linuxbridge-agent.service
12）验证
在Controller节点查看
openstack compute service list
***如果出现未能映射主机host错误时执行下面命令（Controller节点）***
su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova
*****做好快照工作*****

---------------------------------------------------分割线--------------------------------------------------------------

12、Cinder后端存储NFS
##################NFS服务器##################
1）安装NFS软件包
yum install -y nfs-utils
2）安全设置
systemctl stop firewalld
systemctl disable firewalld
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
3）创建NFS共享目录
mkdir /data
4）设置NFS共享
echo '/data 192.168.30.0/24(rw)'>/etc/exports
5)对共享目录修改权限
chmod o+w /data/
6)共享查看
exportfs -rv
7）启动服务
systemctl start nfs rpcbind

##################Compute1节点##################
1）验证nfs服务器是否可用
showmount -e 192.168.30.51
2）修改cinder.conf文件
openstack-config --set  /etc/cinder/cinder.conf    DEFAULT   enabled_backends   lvm,ssd,nfs
echo '[nfs]
volume_driver = cinder.volume.drivers.nfs.NfsDriver
nfs_shares_config = /etc/cinder/nfs_shares
volume_backend_name = nfs' >>/etc/cinder/cinder.conf

echo '192.168.30.51:/data'>>/etc/cinder/nfs_shares
3)重启服务
systemctl restart openstack-cinder-volume.service
---------------------------------------------------分割线--------------------------------------------------------------

13、添加ObjectStorage节点
##################ObjectStorage节点##################
1）配置网络
  sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens33
echo 'IPADDR="192.168.30.61"
NETMASK="255.255.255.0"
GATEWAY="192.168.30.2"
DNS1="192.168.30.2"
DNS2="114.114.114.114"' >> /etc/sysconfig/network-scripts/ifcfg-ens33
  systemctl restart network
2）主机名修改：
  hostnamectl set-hostname objectstorage2
3）安全设置：
 a）关闭firewalld：
     systemctl stop firewalld
     systemctl disable firewalld
  b）Selinux需要关闭及禁用
    setenforce 0
    sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
4）hosts文件映射：
    echo "192.168.30.11     controller">>/etc/hosts
    echo "192.168.30.31     compute1">>/etc/hosts
    echo "192.168.30.41     compute2">>/etc/hosts
    echo "192.168.30.61     objectstorage2">>/etc/hosts（所有节点同步）
5）ntp时间同步（很重要）：
sed -i '4,6d' /etc/chrony.conf
sed -i 's/0.centos.pool.ntp.org/controller/g' /etc/chrony.conf
systemctl restart chronyd
测试时间是否同步：
chronyc sources
**date命令查看两台虚拟机时间是否同步**
6）tar包上传（两台虚拟机都需要上传Openstack安装包）：
通过xshell文件传输把tar包上传至Centos虚拟机上。
并ll查看是否上传完毕（查看文件大小是否跟源文件一致）。
把上传的文件解压到指定目录（根目录）：
tar -zxvf openstack-pike_aliyun.tar.gz -C /
7）openstack 平台所需yum源配置：
注意：原有的yum源不要动，添加一个local.repo文件
echo '[aliyun-os]
name=aliyun-os
baseurl=https://mirrors.aliyun.com/centos/7/os/x86_64/
gpgcheck=0
[openstack]
name=openstack
baseurl=file:///opt/Aliyun-pike/
gpgcheck=0' >/etc/yum.repos.d/local.repo

yum clean all
yum makecache
8）验证以上步骤（注意）

1）添加两块硬盘并扫描
echo "- - -" >>/sys/class/scsi_host/host0/scan
2）格式化磁盘
mkfs.xfs /dev/sdb
mkfs.xfs /dev/sdc
3）创建挂载点并挂载
mkdir -p /srv/node/sdb
mkdir -p /srv/node/sdc
mount /dev/sdb /srv/node/sdb
mount /dev/sdc  /srv/node/sdc
4)开机自动挂载
echo '/dev/sdb /srv/node/sdb xfs noatime,nodiratime,nobarrier,logbufs=8 0 2
/dev/sdc /srv/node/sdc xfs noatime,nodiratime,nobarrier,logbufs=8 0 2'>>/etc/fstab 
5)安装软件包
yum install -y xfsprogs rsync
6)配置同步软件的配置文件
echo '
uid = swift
gid = swift
log file = /var/log/rsyncd.log
pid file = /var/run/rsyncd.pid
address = 192.168.30.61

[account]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/account.lock

[container]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/container.lock

[object]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/object.lock' >>/etc/rsyncd.conf 
7)启动相关服务
systemctl enable rsyncd.service
systemctl start rsyncd.service

8）安装软件包
yum install  -y openstack-swift-account openstack-swift-container  openstack-swift-object
8）修改配置文件
    从compute1节点复制配置文件
   scp /etc/swift/account-server.conf  /etc/swift/container-server.conf /etc/swift/object-server.conf root@192.168.30.61:/etc/swift/（该命令在compute1节点执行）
   修改三个文件中对应IP地址修改本机地址
9）修改挂载点目录权限（所属主、所属组）
chown -R swift:swift /srv/node
10）创建缓存目录及设置权限
mkdir -p /var/cache/swift
chown -R root:swift /var/cache/swift
chmod -R 775 /var/cache/swift
删除所有节点的Ring文件
rm -rf /etc/swift/*.gz /etc/swift/*.builder
##################Controller节点##################

1)创建账户Ring文件，并把所有存储节点添加到Ring中
 cd  /etc/swift
swift-ring-builder account.builder create 10 2 1
swift-ring-builder account.builder  add --region 1 --zone 1 --ip 192.168.30.31 --port 6202  --device sdd --weight 100
swift-ring-builder account.builder  add --region 1 --zone 1 --ip 192.168.30.31 --port 6202  --device sde --weight 100
swift-ring-builder account.builder  add --region 1 --zone 1 --ip 192.168.30.61 --port 6202  --device sdb --weight 100
swift-ring-builder account.builder  add --region 1 --zone 1 --ip 192.168.30.61 --port 6202  --device sdc --weight 100
swift-ring-builder account.builder rebalance
swift-ring-builder account.builder
2）创建容器Ring文件，并把所有存储节点添加到Ring中
swift-ring-builder container.builder create 10 2 1
swift-ring-builder container.builder add --region 1 --zone 1 --ip 192.168.30.31 --port 6201 --device sdd --weight 100
swift-ring-builder container.builder add  --region 1 --zone 1 --ip 192.168.30.31 --port 6201 --device sde --weight 100
swift-ring-builder container.builder add --region 1 --zone 1 --ip 192.168.30.61 --port 6201 --device sdb --weight 100
swift-ring-builder container.builder add  --region 1 --zone 1 --ip 192.168.30.61 --port 6201 --device sdc --weight 100
swift-ring-builder container.builder rebalance
swift-ring-builder container.builder
3）创建对象Ring文件，并把所有存储节点添加到Ring中
swift-ring-builder object.builder create 10 2 1
swift-ring-builder object.builder add  --region 1 --zone 1 --ip 192.168.30.31 --port 6200 --device sdd --weight 100
swift-ring-builder object.builder add  --region 1 --zone 1 --ip 192.168.30.31 --port 6200 --device sde --weight 100
swift-ring-builder object.builder add  --region 1 --zone 1 --ip 192.168.30.61 --port 6200 --device sdb --weight 100
swift-ring-builder object.builder add  --region 1 --zone 1 --ip 192.168.30.61 --port 6200 --device sdc --weight 100
swift-ring-builder object.builder rebalance
swift-ring-builder object.builder
4）把所有生成的账户文件拷贝到存储节点上
scp *.gz root@compute1:/etc/swift/
scp *.gz root@objectstorage2:/etc/swift/
5）把该文件拷贝到所有代理节点及存储节点上
scp /etc/swift/swift.conf root@objectstorage2:/etc/swift/
7）修改配置文件权限
chown -R root:swift /etc/swift
8）启动相关服务
systemctl restart openstack-swift-proxy.service 

##################Storage节点##################
1）修改配置文件目录权限
chown -R root:swift /etc/swift
2）启动相关服务
systemctl enable openstack-swift-account.service openstack-swift-account-auditor.service  openstack-swift-account-reaper.service openstack-swift-account-replicator.service
systemctl start openstack-swift-account.service openstack-swift-account-auditor.service  openstack-swift-account-reaper.service openstack-swift-account-replicator.service
systemctl enable openstack-swift-container.service  openstack-swift-container-auditor.service openstack-swift-container-replicator.service  openstack-swift-container-updater.service
systemctl start openstack-swift-container.service  openstack-swift-container-auditor.service openstack-swift-container-replicator.service  openstack-swift-container-updater.service
systemctl enable openstack-swift-object.service openstack-swift-object-auditor.service  openstack-swift-object-replicator.service openstack-swift-object-updater.service
systemctl start openstack-swift-object.service openstack-swift-object-auditor.service  openstack-swift-object-replicator.service openstack-swift-object-updater.service

##################Compute1节点##################
1）修改配置文件目录权限
chown -R root:swift /etc/swift
2）启动相关服务
systemctl restart openstack-swift-account.service openstack-swift-account-auditor.service  openstack-swift-account-reaper.service openstack-swift-account-replicator.service
systemctl restart openstack-swift-container.service  openstack-swift-container-auditor.service openstack-swift-container-replicator.service  openstack-swift-container-updater.service
systemctl restart openstack-swift-object.service openstack-swift-object-auditor.service  openstack-swift-object-replicator.service openstack-swift-object-updater.service

##################Controller节点##################
验证
swift stat

*****************************************六、排查问题思路*****************************************
1、报错后查看相应的日志文件
 /var/log/服务名/内部组件.log
举个例子：
nova：
/var/log/nova/nova-api.log
日志文件中INFO，WARNING信息是不需要查看
日志中有ERROR信息，说明你的组件有问题，需要排查错误，down
2）查看我们的服务是否是正常（如果不是启动状态，想办法启动起来）
3）每个环节做完以后一定记得做服务检查，最好做个快照。
4）基础环境一定正常


---------------------------------------------------分割线--------------------------------------------------------------



openstack endpoint create --region RegionOne \
orchestration public http://controller:8004/v1/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
orchestration internal http://controller:8004/v1/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
orchestration admin http://controller:8004/v1/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
cloudformation public http://controller:8000/v1
openstack endpoint create --region RegionOne \
cloudformation internal http://controller:8000/v1
openstack endpoint create --region RegionOne \
cloudformation admin http://controller:8000/v1
systemctl enable openstack-heat-api.service \
openstack-heat-api-cfn.service openstack-heat-engine.service
systemctl start openstack-heat-api.service \
openstack-heat-api-cfn.service openstack-heat-engine.service
